defaults:
  - trainer: maxent
  - expert: synthetic
  - policy: softelim
  - prior: maxent
  - hydra: default
  - wandb: default
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled
  - override trainer/policy_trainer: reinforce
  - override trainer/prior_trainer: default
  - _self_

prior:
  num_actions: 100

expert:
  prior:
    name: beta
    num_actions: ${prior.num_actions}
    init_alpha: ${eval:'[20.0, 15.0, 25.0] + [2.0] * (${prior.num_actions} - 3)'}
    init_beta: ${eval:'[2.0] * 3 + [15.0] * (${prior.num_actions} - 3)'}


trainer:
  policy_trainer:
    lr: 1e-2
    mc_samples: 2048
    epochs: 1500
    batch_size: 2048

  prior_trainer:
    lr: 1e-2
    mc_samples: 2048
    batch_size: 2048
    epochs: 1000

  test_horizon: 300
  train_horizon: 100

save_every_steps: 100
keep_every_steps: 1000

seed: 1234
out_dir: output/runs

test_run: False
